# -*- coding: utf-8 -*-
"""optimizers_inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hKArrosmSfd0Dhpde1W7JlNDKzq0Su_r
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/optimizers_results

# Commented out IPython magic to ensure Python compatibility.
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms

import time
import numpy as np
import random
import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import pandas as pd

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f'Using device: {device}')

# Ensure reproducibility
def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    # Ensures deterministic behavior
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# Data augmentation and normalization
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465),
                         (0.2023, 0.1994, 0.2010)),
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465),
                         (0.2023, 0.1994, 0.2010)),
])

# Load training and test datasets
trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform_train)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,
                                          shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform_test)
testloader = torch.utils.data.DataLoader(testset, batch_size=100,
                                         shuffle=False, num_workers=2)

!pip install torch_optimizer

import torch_optimizer

import torchvision.models as models

criterion = nn.CrossEntropyLoss()

# Function to initialize weights
def weights_init(m):
    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
        nn.init.kaiming_normal_(m.weight)
        if m.bias is not None:
            nn.init.constant_(m.bias, 0)

# # Function to get optimizer with the same hyperparameters
# def get_optimizer(name, parameters, lr=0.01, weight_decay=5e-4):
#     if name == 'SGD':
#         return optim.SGD(parameters, lr=lr, momentum=0.9, weight_decay=weight_decay)
#     elif name == 'Adam':
#         return optim.Adam(parameters, lr=lr, weight_decay=weight_decay)
#     elif name == 'RMSProp':
#         return optim.RMSprop(parameters, lr=lr, momentum=0.9, weight_decay=weight_decay)
#     elif name == 'AdaGrad':
#         return optim.Adagrad(parameters, lr=lr, weight_decay=weight_decay)
#     else:
#         raise ValueError(f"Unknown optimizer name: {name}")

def train(net, trainloader, optimizer):
    net.train()
    running_loss = 0.0
    total = 0
    correct = 0

    start_time = time.time()

    for batch_idx, (inputs, targets) in enumerate(trainloader):
        inputs, targets = inputs.to(device), targets.to(device)

        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, targets)
        loss.backward(create_graph=True)
        optimizer.step()

        # Statistics
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

    end_time = time.time()
    epoch_time = end_time - start_time

    train_loss = running_loss / len(trainloader)
    train_acc = 100. * correct / total
    return train_loss, train_acc, epoch_time

def test(net, testloader):
    net.eval()
    running_loss = 0.0
    total = 0
    correct = 0

    all_preds = []
    all_targets = []

    with torch.no_grad():
        for inputs, targets in testloader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = net(inputs)
            loss = criterion(outputs, targets)

            # Statistics
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

            # Collect predictions and targets for metrics
            all_preds.extend(predicted.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())

    test_loss = running_loss / len(testloader)
    test_acc = 100. * correct / total

    return test_loss, test_acc



# Hyperparameters
num_epochs = 50  # Adjust as needed
num_runs = 5  # Number of runs
lr = 0.01  # Same learning rate for all optimizers
weight_decay = 5e-4  # Same weight decay for all optimizers

results_list = []

for run in range(num_runs):
    print(f'\nRun {run+1}/{num_runs}')
    # Set seed for reproducibility (optional)
    set_seed(42 + run)  # Change seed in each run for variability

    for opt_name in ['AdaHessian']:
        print(f'\nTraining with {opt_name} optimizer:')
        # Initialize the model
        net = models.resnet18()
        net.fc = nn.Linear(net.fc.in_features, 10)  # Adjust the final layer for 10 classes
        net.apply(weights_init)  # Reset weights
        net = net.to(device)

        # Initialize the optimizer
        # optimizer = get_optimizer(opt_name, net.parameters(), lr=lr, weight_decay=weight_decay)
        optimizer = torch_optimizer.Adahessian(
            net.parameters(),
            lr= 1.0,
            betas= (0.9, 0.999),
            eps= 1e-4,
            weight_decay=0.0,
            hessian_power=1.0,
            )

        for epoch in range(num_epochs):
            train_loss, train_acc, epoch_time = train(net, trainloader, optimizer)
            test_loss, test_acc = test(net, testloader)

            # Store metrics
            epoch_result = {
                'optimizer': opt_name,
                'run': run + 1,
                'epoch': epoch + 1,
                'epoch_time': epoch_time,
                'train_loss': train_loss,
                'train_acc': train_acc,
                'test_loss': test_loss,
                'test_acc': test_acc
            }
            results_list.append(epoch_result)

            # Optional: Implement learning rate scheduler here

            # Print progress every few epochs
            if (epoch + 1) % 10 == 0 or epoch == 0:
                print(f'Epoch {epoch + 1}/{num_epochs} - '
                      f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - '
                      f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')
        results_df = pd.DataFrame(results_list)
        results_df.to_csv('training_results_adahesian.csv', index=False)
        print("\nResults have been saved to 'training_results_adahesian.csv'")

# After all runs, save results to file
results_df = pd.DataFrame(results_list)
results_df.to_csv('training_results_adahesian.csv', index=False)
print("\nResults have been saved to 'training_results_adahesian.csv'")

!git clone https://github.com/AGI-Arena/MARS.git

# %cd /content/MARS/MARS/optimizers

from MARS.MARS.optimizers.mars import MARS

!git clone https://github.com/LIJUNYI95/SuperAdam

from SuperAdam.SuperAdam import SuperAdam

# Hyperparameters
num_epochs = 50  # Adjust as needed
num_runs = 5  # Number of runs
lr = 0.01  # Same learning rate for all optimizers
weight_decay = 5e-4  # Same weight decay for all optimizers

results_list = []

for run in range(num_runs):
    print(f'\nRun {run+1}/{num_runs}')
    # Set seed for reproducibility (optional)
    set_seed(42 + run)  # Change seed in each run for variability

    for opt_name in ['SuperAdam']: #['MARS']:
        print(f'\nTraining with {opt_name} optimizer:')
        # Initialize the model
        net = models.resnet18()
        net.fc = nn.Linear(net.fc.in_features, 10)  # Adjust the final layer for 10 classes
        net.apply(weights_init)  # Reset weights
        net = net.to(device)

        # Initialize the optimizer
        # optimizer = get_optimizer(opt_name, net.parameters(), lr=lr, weight_decay=weight_decay)
        learning_rate = 1e-3
        betas = (0.9, 0.95)
        gamma = 0.025

        # optimizer = MARS(net.parameters(), lr=learning_rate, betas=betas, gamma=gamma)
        optimizer = SuperAdam(net.parameters(), k=3, c=40, m=2400, gamma=0.003, beta=0.999)

        for epoch in range(num_epochs):
            train_loss, train_acc, epoch_time = train(net, trainloader, optimizer)
            test_loss, test_acc = test(net, testloader)

            # Store metrics
            epoch_result = {
                'optimizer': opt_name,
                'run': run + 1,
                'epoch': epoch + 1,
                'epoch_time': epoch_time,
                'train_loss': train_loss,
                'train_acc': train_acc,
                'test_loss': test_loss,
                'test_acc': test_acc
            }
            results_list.append(epoch_result)

            # Optional: Implement learning rate scheduler here

            # Print progress every few epochs
            if (epoch + 1) % 10 == 0 or epoch == 0:
                print(f'Epoch {epoch + 1}/{num_epochs} - '
                      f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - '
                      f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')
        results_df = pd.DataFrame(results_list)
        results_df.to_csv('training_results_mars.csv', index=False)
        print("\nResults have been saved to 'training_results_mars.csv'")

# After all runs, save results to file
results_df = pd.DataFrame(results_list)
results_df.to_csv('training_results_mars.csv', index=False)
print("\nResults have been saved to 'training_results_mars.csv'")

